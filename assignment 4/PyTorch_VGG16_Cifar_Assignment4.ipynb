{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch VGG16 Cifar Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNbULH6qhaLRzQmf+dwS1Nl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chettkulkarni/deep_learning/blob/master/assignment%204/PyTorch_VGG16_Cifar_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCngKv7ovBxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXrkO2NAwN87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # fully conected layers:\n",
        "        self.fc6 = nn.Linear(2048, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, 1000)\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.dropout(x, 0.5, training=training)\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.dropout(x, 0.5, training=training)\n",
        "        x = self.fc8(x)\n",
        "        return x\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPyrAJIv1jUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(train_batch_size, test_batch_size):\n",
        "    # Fetch training data: total 60000 samples\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR100('data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize((32, 32)),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    # Fetch test data: total 10000 samples\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR100('data', train=False, transform=transforms.Compose([\n",
        "            transforms.Resize((32, 32)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "    return (train_loader, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyJpgOnk0lhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, epoch, train_loader, log_interval):\n",
        "    model.train()\n",
        "\n",
        "    # define loss function\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Iterate over batches of data\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Wrap the input and target output in the `Variable` wrapper\n",
        "        data,target = images.to(device),labels.to(device)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        # Clear the gradients, since PyTorch accumulates them\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        output = model(data)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters(weight,bias)\n",
        "        optimizer.step()\n",
        "\n",
        "        # print log\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train set, Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader),\n",
        "                loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM7j1nC-0yG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, epoch, test_loader):\n",
        "    # State that you are testing the model; this prevents layers e.g. Dropout to take effect\n",
        "    model.eval()\n",
        "\n",
        "    # Init loss & correct prediction accumulators\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # define loss function\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(size_average=False)\n",
        "\n",
        "    # Iterate over data\n",
        "    for data, target in test_loader:\n",
        "        data,target = images.to(device),labels.to(device)\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        \n",
        "        # Forward propagation\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate & accumulate loss\n",
        "        test_loss += loss_fn(output, target).item()\n",
        "\n",
        "        # Get the index of the max log-probability (the predicted output label)\n",
        "        pred = np.argmax(output.cpu().data, axis=1)\n",
        "\n",
        "        # If correct, increment correct prediction accumulator\n",
        "        correct = correct + np.equal(pred, target.cpu().data).sum()\n",
        "\n",
        "    # Print log\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set, Epoch {} , Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW93rLGt017W",
        "colab_type": "code",
        "outputId": "9d4f335c-5f87-4dd1-d3e9-b4bd94afebcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model = VGG16()\n",
        "model.cuda()\n",
        "lr = 0.01\n",
        "momentum=0.5\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 1000\n",
        "train_loader, test_loader = load_data(train_batch_size, test_batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLG604E__2Q9",
        "colab_type": "code",
        "outputId": "928fef58-728e-4672-887b-a0db3b92c01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 10\n",
        "log_interval = 100\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, optimizer, epoch, train_loader, log_interval=log_interval)\n",
        "    test(model, epoch, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set, Epoch 1 [0/50000 (0%)]\tLoss: 0.078902\n",
            "Train set, Epoch 1 [6400/50000 (13%)]\tLoss: 0.098494\n",
            "Train set, Epoch 1 [12800/50000 (26%)]\tLoss: 0.094536\n",
            "Train set, Epoch 1 [19200/50000 (38%)]\tLoss: 0.101996\n",
            "Train set, Epoch 1 [25600/50000 (51%)]\tLoss: 0.090685\n",
            "Train set, Epoch 1 [32000/50000 (64%)]\tLoss: 0.092112\n",
            "Train set, Epoch 1 [38400/50000 (77%)]\tLoss: 0.087513\n",
            "Train set, Epoch 1 [44800/50000 (90%)]\tLoss: 0.091019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set, Epoch 1 , Average loss: 0.0108, Accuracy: 596/10000 (6%)\n",
            "\n",
            "Train set, Epoch 2 [0/50000 (0%)]\tLoss: 0.161239\n",
            "Train set, Epoch 2 [6400/50000 (13%)]\tLoss: 0.074130\n",
            "Train set, Epoch 2 [12800/50000 (26%)]\tLoss: 0.101728\n",
            "Train set, Epoch 2 [19200/50000 (38%)]\tLoss: 0.116841\n",
            "Train set, Epoch 2 [25600/50000 (51%)]\tLoss: 0.083146\n",
            "Train set, Epoch 2 [32000/50000 (64%)]\tLoss: 0.090985\n",
            "Train set, Epoch 2 [38400/50000 (77%)]\tLoss: 0.101729\n",
            "Train set, Epoch 2 [44800/50000 (90%)]\tLoss: 0.093101\n",
            "\n",
            "Test set, Epoch 2 , Average loss: 0.0058, Accuracy: 610/10000 (6%)\n",
            "\n",
            "Train set, Epoch 3 [0/50000 (0%)]\tLoss: 0.092427\n",
            "Train set, Epoch 3 [6400/50000 (13%)]\tLoss: 0.054723\n",
            "Train set, Epoch 3 [12800/50000 (26%)]\tLoss: 0.032961\n",
            "Train set, Epoch 3 [19200/50000 (38%)]\tLoss: 3.400541\n",
            "Train set, Epoch 3 [25600/50000 (51%)]\tLoss: 0.034751\n",
            "Train set, Epoch 3 [32000/50000 (64%)]\tLoss: 0.013592\n",
            "Train set, Epoch 3 [38400/50000 (77%)]\tLoss: 0.001149\n",
            "Train set, Epoch 3 [44800/50000 (90%)]\tLoss: 0.000413\n",
            "\n",
            "Test set, Epoch 3 , Average loss: 0.0004, Accuracy: 639/10000 (6%)\n",
            "\n",
            "Train set, Epoch 4 [0/50000 (0%)]\tLoss: 0.001293\n",
            "Train set, Epoch 4 [6400/50000 (13%)]\tLoss: 0.017035\n",
            "Train set, Epoch 4 [12800/50000 (26%)]\tLoss: 0.000165\n",
            "Train set, Epoch 4 [19200/50000 (38%)]\tLoss: 0.000155\n",
            "Train set, Epoch 4 [25600/50000 (51%)]\tLoss: 0.000174\n",
            "Train set, Epoch 4 [32000/50000 (64%)]\tLoss: 0.000083\n",
            "Train set, Epoch 4 [38400/50000 (77%)]\tLoss: 0.000176\n",
            "Train set, Epoch 4 [44800/50000 (90%)]\tLoss: 0.000014\n",
            "\n",
            "Test set, Epoch 4 , Average loss: 0.0000, Accuracy: 640/10000 (6%)\n",
            "\n",
            "Train set, Epoch 5 [0/50000 (0%)]\tLoss: 0.000040\n",
            "Train set, Epoch 5 [6400/50000 (13%)]\tLoss: 0.000034\n",
            "Train set, Epoch 5 [12800/50000 (26%)]\tLoss: 0.000044\n",
            "Train set, Epoch 5 [19200/50000 (38%)]\tLoss: 0.000049\n",
            "Train set, Epoch 5 [25600/50000 (51%)]\tLoss: 0.000013\n",
            "Train set, Epoch 5 [32000/50000 (64%)]\tLoss: 0.000005\n",
            "Train set, Epoch 5 [38400/50000 (77%)]\tLoss: 0.000027\n",
            "Train set, Epoch 5 [44800/50000 (90%)]\tLoss: 0.000334\n",
            "\n",
            "Test set, Epoch 5 , Average loss: 0.0001, Accuracy: 640/10000 (6%)\n",
            "\n",
            "Train set, Epoch 6 [0/50000 (0%)]\tLoss: 0.000706\n",
            "Train set, Epoch 6 [6400/50000 (13%)]\tLoss: 0.000311\n",
            "Train set, Epoch 6 [12800/50000 (26%)]\tLoss: 0.000241\n",
            "Train set, Epoch 6 [19200/50000 (38%)]\tLoss: 0.000207\n",
            "Train set, Epoch 6 [25600/50000 (51%)]\tLoss: 0.000245\n",
            "Train set, Epoch 6 [32000/50000 (64%)]\tLoss: 0.000224\n",
            "Train set, Epoch 6 [38400/50000 (77%)]\tLoss: 0.000030\n",
            "Train set, Epoch 6 [44800/50000 (90%)]\tLoss: 0.000239\n",
            "\n",
            "Test set, Epoch 6 , Average loss: 0.0000, Accuracy: 640/10000 (6%)\n",
            "\n",
            "Train set, Epoch 7 [0/50000 (0%)]\tLoss: 0.000075\n",
            "Train set, Epoch 7 [6400/50000 (13%)]\tLoss: 0.000026\n",
            "Train set, Epoch 7 [12800/50000 (26%)]\tLoss: 0.000144\n",
            "Train set, Epoch 7 [19200/50000 (38%)]\tLoss: 0.000079\n",
            "Train set, Epoch 7 [25600/50000 (51%)]\tLoss: 0.000089\n",
            "Train set, Epoch 7 [32000/50000 (64%)]\tLoss: 0.000054\n",
            "Train set, Epoch 7 [38400/50000 (77%)]\tLoss: 0.000015\n",
            "Train set, Epoch 7 [44800/50000 (90%)]\tLoss: 0.000028\n",
            "\n",
            "Test set, Epoch 7 , Average loss: 0.0000, Accuracy: 640/10000 (6%)\n",
            "\n",
            "Train set, Epoch 8 [0/50000 (0%)]\tLoss: 0.000033\n",
            "Train set, Epoch 8 [6400/50000 (13%)]\tLoss: 0.000057\n",
            "Train set, Epoch 8 [12800/50000 (26%)]\tLoss: 0.000023\n",
            "Train set, Epoch 8 [19200/50000 (38%)]\tLoss: 0.000019\n",
            "Train set, Epoch 8 [25600/50000 (51%)]\tLoss: 0.000013\n",
            "Train set, Epoch 8 [32000/50000 (64%)]\tLoss: 0.000006\n",
            "Train set, Epoch 8 [38400/50000 (77%)]\tLoss: 0.000014\n",
            "Train set, Epoch 8 [44800/50000 (90%)]\tLoss: 0.000017\n",
            "\n",
            "Test set, Epoch 8 , Average loss: 0.0000, Accuracy: 640/10000 (6%)\n",
            "\n",
            "Train set, Epoch 9 [0/50000 (0%)]\tLoss: 0.000030\n",
            "Train set, Epoch 9 [6400/50000 (13%)]\tLoss: 0.000012\n",
            "Train set, Epoch 9 [12800/50000 (26%)]\tLoss: 0.000012\n",
            "Train set, Epoch 9 [19200/50000 (38%)]\tLoss: 0.000346\n",
            "Train set, Epoch 9 [25600/50000 (51%)]\tLoss: 0.000021\n",
            "Train set, Epoch 9 [32000/50000 (64%)]\tLoss: 0.000003\n",
            "Train set, Epoch 9 [38400/50000 (77%)]\tLoss: 0.000009\n",
            "Train set, Epoch 9 [44800/50000 (90%)]\tLoss: 0.000032\n",
            "\n",
            "Test set, Epoch 9 , Average loss: 0.0000, Accuracy: 640/10000 (6%)\n",
            "\n",
            "Train set, Epoch 10 [0/50000 (0%)]\tLoss: 0.000401\n",
            "Train set, Epoch 10 [6400/50000 (13%)]\tLoss: 0.000062\n",
            "Train set, Epoch 10 [12800/50000 (26%)]\tLoss: 0.000625\n",
            "Train set, Epoch 10 [19200/50000 (38%)]\tLoss: 0.000210\n",
            "Train set, Epoch 10 [25600/50000 (51%)]\tLoss: 0.000104\n",
            "Train set, Epoch 10 [32000/50000 (64%)]\tLoss: 0.002864\n",
            "Train set, Epoch 10 [38400/50000 (77%)]\tLoss: 0.000040\n",
            "Train set, Epoch 10 [44800/50000 (90%)]\tLoss: 0.000045\n",
            "\n",
            "Test set, Epoch 10 , Average loss: 0.0000, Accuracy: 640/10000 (6%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsoVALRVAWZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}